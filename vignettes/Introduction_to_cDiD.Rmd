---
title: "Introduction to cDiD with Multiple Time Periods"
author: "Joel Cuerrier"
date:  "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to cDiD with Multiple Time Periods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

```{r setup, include = FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com"))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
install.packages("pandoc")
```

<!-- rmarkdown::pandoc_version() -->

```{r, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Attribution of Vignettes to Original Author
The vignettes included in this library are based on those authored by [Brantly Callaway and Pedro H.C. Sant'Anna] for the 'did' package. We have adapted the content for use in this library to provide users with accessible and informative documentation. The original vignettes authored by [Brantly Callaway and Pedro H.C. Sant'Anna] can be found in the 'did' package documentation [https://bcallaway11.github.io/did/articles/did-basics.html].

# Introduction 
****
* This vignette provides an introductory overview of utilizing Chained-Difference-in-Differences (cDiD) designs to identify and estimate the average impact of engaging in a treatment, with a specific emphasis on leveraging functionalities from the cdid package. 

* The package is based on the "did" package developed by Brantly Callaway and Pedro H.C. Sant'Anna  (2020) and extends its capabilities to accommodate multiple time periods, variations in treatment timing, treatment effect heterogeneity, general missing data patterns, and sample selection observable. 

* As a result, the output generated by  the 'cdid' library mirrors that of the did package. The outputs produced by my 'cdid' align seamlessly with those of the 'did' package. Examples demonstrating this compatibility are provided in the documentation for reference.

* The cdid package allows for multiple periods, variation in treatment timing, treatment effect heterogeneity, general missing data patterns, and sample selection on observables.

* The chained DiD rests on a parallel trends assumption, conditioned on being sampled. 

* The chained DiD is robust to some forms of attrition caused by unobservable heterogeneity.

* We identify three main advantages of our approach: (1) it does not require having a balanced panel subsample as it is the case with a standard DiD; (2) identifying assumption allows for sample selection on time-persistent unobservable (or latent) factors, unlike the cross-section DiD which treats the sample as repeated cross-sectional data; and (3) it may also deliver efficiency gains compared to existing methods, notably when the outcome variable is highly time-persistent.

* The cdid package is designed to provide group-time average treatment effects. Future updates to the package will expand its capabilities to include event-study type estimates, which encompass treatment effect parameters corresponding to various durations of exposure to the treatment, as well as overall treatment effect estimates.

# Examples with numerical simulations
****
We propose a simulation design adapted from the first section. Let us specify the
potential outcome as a components of variance:

$$
Y_{it}(D_i) = \alpha_i + \delta_t + \sum_{\tau=2}^{t} \beta_{\tau} D_{i\tau} + \varepsilon_{it}
$$

Where $D_{i\tau} ∈ {0, 1}$ denotes whether individual i has been treated in $\tau$ or earlier. Let us assume that $\tau ∈ {0, ..., T + 1}$ and treatments can only occur in t ≥ 2 so that G ∈ {2, ..., T + 1}. The data generating process is characterized by the following assumptions:

* The individual-specific unobservable heterogeneity is iid gaussian: $\alpha_i ∼ N(1, \alpha^2_{\alpha})$, where $\alpha^2 = 2;$
* The time-specific unobservable heterogeneity is iid gaussian: $\delta_t ∼ N(1, 1)$;
* The treatment effect is constant over time: $\beta_{\tau} = 1$;
* The error term is iid gaussian: $\varepsilon_{it} ∼ N(0, \alpha^2_{\varepsilon})$;
* The probability to receive the treatment at time g, conditional on being treated at g or in the control group, is defined as: 
$$Pr(G_{ig} = 1|X_i, \alpha_i, G_{ig} + C_i = 1) = \frac{1}{1 + \exp(\theta_0 + \theta_1 X_i + \theta_2 \alpha_i \times g)}$$.
Where $X_i ∼ N(1, 1)$ is observable for every i, unlike $\alpha_i$, and $\theta_0 = −1, \theta_1 = 0.4$ and $\theta_2 = 0$ or $\theta_2 = 0.2$. In the latter case, the treatment probability varies with treatment timing and the unobserved individual heterogeneity;
* The sampling probability in the consecutive periods t, t + 1 conditional on $\alpha_i$ is given by:
$$Pr((S_{it} + 1 = 1|\alpha_i)) = \frac{1}{1 + \exp(\lambda_0 + \lambda_1 \alpha_i \times t)}$$
With $\lambda_0 = −1$, and $\lambda_1 = 0$ or $\lambda_1 = 0.2$, so that the sampling process can also vary with time and the unobserved individual heterogeneity.

We simulate the sampled data in two steps. First, we generate a population sample
for each period t to represent individuals that are either treated at t or in the control
group. Second, we sample from this population using the specified process. 

1. Generate a population of individuals
(a) Draw $N = 2 x max $\frac{n}{E_{\alpha}[Pr(S_{itt+1})]}$ individuals per period in order to have $T + 2$ population samples of N individuals, where each individual is characterized by a vector $(\alpha_i, \delta_t, X_i, \epsilon_{it})$.
(b) Separately for each population sample g, draw a uniform random number $(\xi_i \in [0, 1])$ per individual. If $\xi_i \leq Pr(G_{it} = 1|X_i, \alpha_i, G_{it} + C_i = 1)$, then set $(G_{ig} = 1, C_i = 0)$, otherwise set $(G_{ig} = 0, C_i = 1)$.
(c) Compute $Y_{it}$ from $(\alpha_{i}, \delta_{t}, X_{i}, ε_{it}, G_{i0}, ..., _{iT+1}, C_{i})$;

2. Sample from this population
(a) Draw a uniform random number $η_{it} ∈ [0, 1]$ per individual i and period t. If $η_{it} ≤ Pr(S_{itt+1} = 1|\alpha_{i}), then set S_{itt+1} = 1$ and $S_{iττ+1} = 0$ for $τ /neq t$;

(b) Draw (without replacement) n individuals per period t from the population for which $S_{itt+1} = 1$;
(c) Compute the different estimators.
(d) Repeat steps 1(b)-2(c) 1,000 times and report the mean and standard deviation of the estimators.

# Building the dataset
****
You can observe that both libraries utilize the same .R files and are largely compatible with one another.
This is because our cdid library is primarily built upon the did library.
```{r, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
library(devtools)
remotes::install_github("joelcuerrier/cdid", ref = "main", force = TRUE, dependencies=TRUE)
library(cdid)
ls("package:cdid")
```

```{r, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
set.seed(123)
options(warn = -1)

# Using the function `fonction_simu_attrition` from the `cdid` package to generate a simulation dataset.
data=fonction_simu_attrition(theta2_alpha_Gg=0.2, lambda1_alpha_St=0.2, sigma_alpha=2, sigma_epsilon=0.5, alpha_percentile=0.75)
View(data)
# We are fixing the missing observations.


#      id annee  Y1_chaine       X         annee_G   P_Y1_chaine   traite_G  select  
# 6616  1     1  0.7697235   1.3126786       0            0           0         1   
# 6617  2     1  1.4992124   1.6380014       0            0           0         1   
# 6618  3     1  2.4497511   0.1496297       0            0           0         1   
# 6619  4     1 -2.6487611  -0.7435851       0            0           0         1   
# 6620  5     1 -1.1038018   0.4412948       0            0           0         1   
# 6621  6     1 -4.0623397   1.6870456       0            0           0         1   
```

# Code Implementation and Comparative Results

## Difference-in-Differences
```{r}

did.results = did::att_gt(
  yname="Y1_chaine",
  tname="annee",
  idname = "id",
  gname = "annee_G",
  xformla = ~X,
  data = data,
  weightsname = NULL,
  alp = 0.05,
  bstrap = TRUE,
  cband = TRUE,
  biters = 1000,
  clustervars = NULL,
  est_method = "dr",
  base_period = "varying",
  print_details = FALSE,
  pl = FALSE,
  cores = 1
)
did.results

```



## Cross-Sectional
```{r}
did.cs.results = did::att_gt(
  yname="Y1_chaine",
  tname="annee",
  idname = "id",
  gname = "annee_G",
  xformla = ~X,
  data = data,
  weightsname = NULL,
  alp = 0.05,
  bstrap = TRUE,
  cband = TRUE,
  biters = 1000,
  clustervars = NULL,
  est_method = "dr",
  base_period = "varying",
  print_details = FALSE,
  pl = FALSE,
  cores = 1,
  panel = FALSE
)


```

## Chained Difference-in-Differences
The function `chained` estimates the average treatment effect for the chained DiD design. See the documentation here : https://www.davidbenatia.com/publication/chaineddid/.
```{r}
data=fonction_simu_attrition(theta2_alpha_Gg=0, lambda1_alpha_St=0, sigma_alpha=2, sigma_epsilon=0.5,alpha_percentile=0.9)

chained.results=chained(
                yname="Y1_chaine",
                tname="annee",
                idname="id",
                gname="annee_G",
                xformla=~X, 
                propensityformla=c("X"), 
                data=data,      
                anticipation=0,      
                weightsname=c("P_Y1_chaine"), #St   
                weight_assumption=NULL,
                bstrap=FALSE, 
                biters=1000,
                debT=3,
                finT=8,
                deb=1,
                fin=8,
                select='select',
                treated='traite_G',
                pl=FALSE,
                cores=1,
                cband=TRUE,
                clustervars=NULL)
```
```{r}
# Debugging
#Vérification des ATT sans les preprocess. Checked
# Vérification des std.


source("R/fonction_simu_attrition.R")
source("R/fonction_simu_attrition.R")
source("R/fonction_simu_attrition_nofe.R")
source("R/fonctions_estimation_Boot.R")
source("R/mp_spatt_Boot.R")
source("R/compute_mp_spatt_Boot_alt.R")
source("R/panelDiffV.R")
source("R/gg.R")
source("R/agregat.R")
source("R/process_attgt.R")
source("R/pre_process_did.R")
source("R/DIDparams.R")
source("R/mboot.R")
source("R/MP.R")
source("R/chained.R")
source("R/compute.aggte.R")
source("R/aggte.R")
source("R/compute.aggte.R")
data=fonction_simu_attrition(theta2_alpha_Gg=0.2, lambda1_alpha_St=0.2, sigma_alpha=2, sigma_epsilon=0.5,alpha_percentile=0.75)
# data=fonction_simu_attrition_nofe(theta2_alpha_Gg=0, lambda1_alpha_St=0, sigma_alpha=2, sigma_epsilon=0.5)

resultat= chained_estimPeriod_Boot(yname="Y1_chaine",
                tname="annee",
                idname="id",
                gname="annee_G",
                xformla=~X, 
                propensityformla=c("X"), 
                data=data,      
                # anticipation=0,      
                weightsname=c("P_Y1_chaine"), #St   
                weight_assumption=NULL,
                bstrap=FALSE, 
                biters=1000,
                debT=3,
                finT=8,
                deb=1,
                fin=8,
                select='select',
                treated='traite_G',
                # pl=FALSE,
                # cores=1,
                # cband=TRUE,
                # clustervars=NULL)
  )




# Fonction d'influence (agregat_influence)
# influ<-agregat_influence(tab=resultat[[1]][[1]],array_inf=resultat[[2]],listG=resultat[[1]][[3]],nom_outcome=nom_outcome,tname=tname,first.treat.name=first.treat.name,poids=dum)

tname = 'annee'
first.treat.name = 'annee_G'
nom_outcome = c('Y1_chaine')
tab = resultat[[1]][[1]]
array_inf = resultat[[1]][[2]]
listG=resultat[[1]][[3]]
poids = resultat[[2]]
str(array_inf)
  
  nb_an<-unique(tab[,tname])
  nn<-length(nb_an)
  nb_an_long<-c(min(nb_an)-1,nb_an)
  an_cohort<-unique(tab[,first.treat.name])
  NN<-length(an_cohort)
  agreg_influence = array(0,dim=c(dim(array_inf)[1],(1+length(nom_outcome)),nn*NN))

  cumsum_after<-function(array_f){
    for (k in 2:dim(array_f)[3]){
      array_f[,,k]=array_f[,,k]+array_f[,,k-1]
    }
    array_f
  }
  cumsum_befor<-function(array_f){
    len = dim(array_f)[3]
    for (k in 1:(len-1)){
      j=len-k
      array_f[,,j]=array_f[,,j]+array_f[,,j+1]
    }
    array_f[,,1:len]=(-1)*array_f[,,1:len]
  }
  sum_array<-function(array_f){
    len = dim(array_f)[3]
    mat_result=array_f[,,1]
    if (len>1){
    for (k in 2:len){
      mat_result=mat_result+array_f[,,k]
      }
    }
    mat_result
  }

  #delete
  

  ATT=data.frame()
  for (i in 1:NN){
    tab_cohort=tab[(nn*(i-1)+1):(nn*i),]
    after=tab_cohort[tab_cohort[,tname]>=an_cohort[i],]
    befor=tab_cohort[tab_cohort[,tname]<an_cohort[i],]
    after$time_to_treatment=after[,tname]-after[,first.treat.name]
    befor$time_to_treatment=befor[,tname]-befor[,first.treat.name]-1
    after$compteur
    befor$compteur
    
    influence_cohort_after=array_inf[,,after$compteur]
    influence_cohort_befor=array_inf[,,befor$compteur]
    if (is.na(dim(influence_cohort_after)[3])==TRUE){
      influence_cohort_after=array_inf[,,after$compteur]
    } else if (dim(influence_cohort_after)[3]==0) {
      influence_cohort_after=array_inf[,,after$compteur]
    } else {
      influence_cohort_after=cumsum_after(influence_cohort_after)
      # Modif
      influence_cohort_after[,1,]=array_inf[,1,after$compteur]
    }
    if (is.na(dim(influence_cohort_befor)[3])==TRUE){
      influence_cohort_befor=array_inf[,,befor$compteur]
    } else if (dim(influence_cohort_befor)[3]==0) {
      influence_cohort_befor=array_inf[,,befor$compteur]
    } else {
      influence_cohort_befor=cumsum_befor(influence_cohort_befor)
      # Modif
      influence_cohort_befor[,1,]=array_inf[,1,befor$compteur]
    }


    agreg_influence[,,after$compteur]<-influence_cohort_after
    agreg_influence[,,befor$compteur]<-influence_cohort_befor
    ATT_temp=rbind(befor,after)
    ATT_temp=merge(ATT_temp,poids,by.x=first.treat.name, by.y="Var1",all.x)
    
    ATT=rbind(ATT,ATT_temp)
  }



  ttt=unique(sort(ATT$time_to_treatment))
  agreg_influence_final = array(0,dim=c(dim(array_inf)[1],(1+length(nom_outcome)),length(ttt)+1))
  for (i in 1:length(ttt)){
    ATTi=ATT[ATT$time_to_treatment==ttt[i],]
    ATTi$Freq=ATTi$Freq/sum(ATTi$Freq)
    agreg_inf_i=agreg_influence[,,ATTi$compteur]
    if (is.na(dim(agreg_inf_i)[3])==TRUE){
      agreg_inf_i=array(0,dim=c(dim(array_inf)[1],(1+length(nom_outcome)),1))
      agreg_inf_i[,,1]=agreg_influence[,,ATTi$compteur]
    } else {
        agreg_inf_i=agreg_influence[,,ATTi$compteur]
        for (j in 1:dim(ATTi)[1]){
          agreg_inf_i[,,j]=ATTi$Freq[j]*agreg_inf_i[,,j]
        }
        # Modif
        agreg_inf_i[,1,]=agreg_influence[,1,ATTi$compteur]
    }
    # D�but modif
    # Comme les poids sont des fr�quences empiriques, ils sont al�atoires, 
    # il faut ajouter un terme suppl�mentaire dans la fonction d'influence pour ne pas sous-�valuer la variance
    for (j in 1:dim(ATTi)[1]){
      traitement_j <- listG
      traitement_j[,1][traitement_j[,1]!= ATTi[j,1]] <- 0
      traitement_j[,1][traitement_j[,1]== ATTi[j,1]] <- 1
      traitement_j[,1] <- (traitement_j[,1]-mean(traitement_j[,1])) * 1/(dim(ATTi)[1])
      # Doute sur le fait que listG & agreg_inf_i ait le m�me nb d'observations
      # mat_gamma <- as.data.frame(agreg_inf_i[,1,j])
      # colnames(mat_gamma) <- c("var1")
      # mat_gamma$gamma <- merge(mat_gamma,traitement_j,by.x="var1", by.y="iden_num",all.x)
      # agreg_inf_i[,(2:dim(agreg_inf_i)[2]),j] <- agreg_inf_i[,(2:dim(agreg_inf_i)[2]),j]+ mat_gamma$gamma%*%ATTi[j,nom_outcome]
      agreg_inf_i[,(2:dim(agreg_inf_i)[2]),j] <- agreg_inf_i[,(2:dim(agreg_inf_i)[2]),j]+ t(t(traitement_j[,1]))%*%as.matrix(ATTi[j,nom_outcome])
    }
    # End modif
    if (ttt[i]< -1 ) {
      agreg_influence_final[,,i]=sum_array(agreg_inf_i)
    } else {
      agreg_influence_final[,,i+1]=sum_array(agreg_inf_i)
    }
  }
  # Remettre les identifiants des individus
  for (i in 1:(length(ttt)+1)){
    agreg_influence_final[,1,i]=array_inf[,1,1]
  }
  
  





```

## Chained difference with Assumption 1 (missing trends)
```{r}
chained.c1.results=chained(
                yname="Y1_chaine",
                tname="annee",
                idname="id",
                gname="annee_G",
                xformla=~X, 
                propensityformla=c("X"), 
                data=data,      
                anticipation=0,      
                weightsname=c("P_Y1_chaine"), #St   
                weight_assumption="missing_trends",
                bstrap=FALSE, 
                biters=1000,
                debT=3,
                finT=8,
                deb=1,
                fin=8,
                select='select',
                treated='traite_G',
                pl=FALSE,
                cores=1,
                cband=TRUE,
                clustervars=NULL)
str(chained.c1.results)
                
```
## GMM 
```{r}
gmm.results=gmm(
                yname="Y1_chaine",
                tname="annee",
                idname="id",
                gname="annee_G",
                xformla=~X, 
                propensityformla=c("X"), 
                data=data,      
                anticipation=0,      
                weightsname=c("P_Y1_chaine"), 
                # weight_assumption="missing_trends", #Not yet implemented.
                weight_assumption=NULL,
                bstrap=FALSE, 
                biters=1000,
                debT=3,
                finT=8,
                deb=1,
                fin=8,
                select='select',
                treated='traite_G',
                pl=FALSE,
                cores=1,
                cband=TRUE,
                clustervars=NULL)
```








```{r, echo=FALSE, results="hide", warning=FALSE, message=FALSE}
plots <- function(type){
  # Setting the caption based on the type
  if (type == "simple"){
    caption <- "Weighted average of all group-time average treatment effects with weights proportional to group size."
  } else if (type == "dynamic"){ 
    caption <- "Average effects across different lengths of exposure to the treatment."
  } else if (type == "group"){
    caption <- "Computes average treatment effects across different groups."
  } else if (type == "calendar"){
    caption <- "Average treatment effects across different time periods."
  } else {
    stop("Invalid type provided.")
  }
  
  # Calculating AGGTE for each method
  agg.es.chained <- aggte(MP = chained.results, type = type)
  agg.es.chained.a1 <- aggte(MP = chained.c1.results, type = type)
  agg.es.did <- aggte(MP = did.results, type = type)
  agg.es.did.cs <- aggte(MP = did.cs.results, type = type)
  agg.es.gmm <- aggte(MP = gmm.results, type = type)
  
  # Overall ATT comparison from DID and chained
  overall_df <- data.frame(
    "overall att did" = agg.es.did$overall.att,
    "overall att did cs" = agg.es.did$overall.att,
    "overall att chained" = agg.es.chained$overall.att,
    "overall att chained a1" = agg.es.chained.a1$overall.att,
    "overall att gmm" = agg.es.gmm$overall.att
  )
  kable(overall_df, format = "html", table.attr = "style='width:100%;'")  # For HTML output
  
  # Group ATT comparison from DID and chained
  group_df <- data.frame(
    
    "gt att did" = agg.es.did$att.egt,
    "gt att cs" = agg.es.did.cs$overall.att,
    "gt att chained" = agg.es.chained$att.egt,
    "gt att chained a1" = agg.es.chained.a1$att.egt,
    "gt att gmm" = agg.es.gmm$att.egt
  )
  kable(group_df, format = "html", table.attr = "style='width:100%;'")  # For HTML output

  if (type != "simple"){  
  # Plotting the results
  plot1 <- ggdid(agg.es.did) + ggtitle("DID")
  plot2 <- ggdid(agg.es.did.cs) + ggtitle("CS")
  plot3 <- ggdid(agg.es.chained) + ggtitle("Chained")
  plot4 <- ggdid(agg.es.chained.a1) + ggtitle("chained.a1")
  plot5 <- ggdid(agg.es.gmm) + ggtitle("GMM")
  
  combined_plot <- (plot1 | plot2 | plot3 | plot4 | plot5) +
    plot_annotation(
      caption = caption,
      theme = theme(plot.caption = element_text(hjust = 0.5)) # Center the caption
    )
  
  ggsave(paste0(type, ".png"), combined_plot, width = 10, height = 4, dpi = 300)
}}
# plots("simple")
plots("dynamic")
plots("group")
plots("calendar")
```

```{r img-with-knitr, echo=FALSE, fig.align='center', out.width='50%', fig.alt='Alternative text for the images'}
knitr::include_graphics(c(here("R/outputs/dynamic.png"), here("R/outputs/group.png"), here("R/outputs/calendar.png")))

browseVignettes("cdid")
```




## Future Improvements will include
- The ability to accountn for the uncertainty in the propensity scores from the first step.
- Efficiency in the GMM funtion compute.


